{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa604d8-3c27-4340-8d33-ce8d623bfda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 16:25:27.322486: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-26 16:25:27.399780: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-26 16:25:27.399923: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-26 16:25:27.402529: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-26 16:25:27.418398: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-26 16:25:27.419254: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-26 16:25:29.562902: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from keras_facenet import FaceNet\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mtcnn\n",
    "import mediapipe as mp\n",
    "from copy import deepcopy\n",
    "from matplotlib import pyplot as plt\n",
    "import copy\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f32852ff-4635-4b1b-b365-fabe1e0bb1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the ml models that are used for the creation of the system\n",
    "Haarcascade=cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "Facenet=FaceNet()\n",
    "eyecascade=cv2.CascadeClassifier(\"haarcascade_eye.xml\")\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f7aa677-a904-4143-a132-2d0856df82d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is  a class that is used for various datamanupulation in our database folder \n",
    "class database_man():\n",
    "    def __init__(self,threshold,database_path):\n",
    "        self.threshold=threshold\n",
    "        self.database_path=database_path\n",
    "        self.list_of_people=os.listdir(self.database_path)\n",
    "        \n",
    "    def return_name_max(self,embedding):\n",
    "        confidence_list=[]\n",
    "        for i in self.list_of_people:\n",
    "            temp_np=np.load(f\"{self.database_path}/{i}\")\n",
    "            confidence_list.append(np.linalg.norm(embedding-temp_np))\n",
    "        Most_sim=min(confidence_list)\n",
    "        return Most_sim,os.path.splitext(self.list_of_people[confidence_list.index(Most_sim)])[0]\n",
    "    def create_numpy(self,name,image_path):\n",
    "        embedding=get_embeddings(image_path)\n",
    "        path=f\"{self.database_path}/{name}\"\n",
    "        np.save(path,embedding)\n",
    "    def create_numpy_frame(self,embedding,name):\n",
    "        path=f\"{self.database_path}/{name}\"\n",
    "        np.save(path,embedding)\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbae4866-86d8-418e-847f-8156bc359c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "database=database_man(0.8,\"DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03cdcb62-c4da-4651-932a-a9ae1b31ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this fuction takes an image path detects face and eyes of the image rotate crops the face, then rotate the croped image such that the eyes \n",
    "#the face is alligned straight\n",
    "def get_embeddings(path):\n",
    "    with mp_face_detection.FaceDetection(\n",
    "            model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(image)\n",
    "        \n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, _ = image.shape\n",
    "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                \n",
    "                cropedimage = image[y:y+h, x:x+w]\n",
    "                \n",
    "                cropimg = cv2.resize(copy.deepcopy(cropedimage), (160, 160))\n",
    "                eyes = eyecascade.detectMultiScale(cropimg, 1.1, 4)\n",
    "\n",
    "                if len(eyes) >= 2:\n",
    "                    eye1 = eyes[0]\n",
    "                    eye2 = eyes[1]\n",
    "\n",
    "                    left_eye_center = (int(eye1[0] + (eye1[2] / 2)), int(eye1[1] + (eye1[3] / 2)))\n",
    "                    right_eye_center = (int(eye2[0] + (eye2[2] / 2)), int(eye2[1] + (eye2[3] / 2)))\n",
    "\n",
    "                    \n",
    "                    angle = np.arctan2(right_eye_center[1] - left_eye_center[1], right_eye_center[0] - left_eye_center[0]) * 180 / np.pi\n",
    "\n",
    "                   \n",
    "                    eyes_center = ((left_eye_center[0] + right_eye_center[0]) // 2, (left_eye_center[1] + right_eye_center[1]) // 2)\n",
    "\n",
    "                    \n",
    "                    M = cv2.getRotationMatrix2D(eyes_center, angle, 1.0)\n",
    "\n",
    "                    \n",
    "                    rotated = cv2.warpAffine(cropimg, M, (160, 160))\n",
    "\n",
    "                    cropimg_tens = np.expand_dims(rotated, axis=0)\n",
    "                    embeddings = Facenet.embeddings(cropimg_tens)\n",
    "                    '''print(embeddings)\n",
    "                    plt.imshow(rotated)  \n",
    "                    plt.show()  '''\n",
    "                \n",
    "                    return embeddings\n",
    "                else:\n",
    "                    \n",
    "                    print(\"Not enough eyes detected for rotation.\")\n",
    "                    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbbead0b-69ab-421e-870e-0b69a59a878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same function but takes frames as input\n",
    "def get_embeddings_frame(frame):\n",
    "    with mp_face_detection.FaceDetection(\n",
    "            model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "        image = frame\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(image)\n",
    "        \n",
    "        if results.detections:\n",
    "            for detection in results.detections:\n",
    "                bboxC = detection.location_data.relative_bounding_box\n",
    "                ih, iw, _ = image.shape\n",
    "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), int(bboxC.width * iw), int(bboxC.height * ih)\n",
    "                \n",
    "                cropedimage = image[y:y+h, x:x+w]\n",
    "                \n",
    "                cropimg = cv2.resize(copy.deepcopy(cropedimage), (160, 160))\n",
    "                eyes = eyecascade.detectMultiScale(cropimg, 1.1, 4)\n",
    "\n",
    "                if len(eyes) >= 2:\n",
    "                    eye1 = eyes[0]\n",
    "                    eye2 = eyes[1]\n",
    "\n",
    "                    left_eye_center = (int(eye1[0] + (eye1[2] / 2)), int(eye1[1] + (eye1[3] / 2)))\n",
    "                    right_eye_center = (int(eye2[0] + (eye2[2] / 2)), int(eye2[1] + (eye2[3] / 2)))\n",
    "\n",
    "                    \n",
    "                    angle = np.arctan2(right_eye_center[1] - left_eye_center[1], right_eye_center[0] - left_eye_center[0]) * 180 / np.pi\n",
    "\n",
    "                   \n",
    "                    eyes_center = ((left_eye_center[0] + right_eye_center[0]) // 2, (left_eye_center[1] + right_eye_center[1]) // 2)\n",
    "\n",
    "                    \n",
    "                    M = cv2.getRotationMatrix2D(eyes_center, angle, 1.0)\n",
    "\n",
    "                    \n",
    "                    rotated = cv2.warpAffine(cropimg, M, (160, 160))\n",
    "\n",
    "                    cropimg_tens = np.expand_dims(rotated, axis=0)\n",
    "                    embeddings = Facenet.embeddings(cropimg_tens)\n",
    "                    '''print(embeddings)\n",
    "                    plt.imshow(rotated)  \n",
    "                    plt.show()  '''\n",
    "                \n",
    "                    return embeddings\n",
    "                else:\n",
    "                    \n",
    "                    print(\"Not enough eyes detected for rotation.\")\n",
    "                    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5f0bf1d-053d-4e99-b169-d7bd4256b298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 256ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "('Philip', 7)\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "('Philip', 5)\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "Not enough eyes detected for rotation.\n",
      "Not enough eyes detected for rotation.\n",
      "('Philip', 4)\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "('Philip', 7)\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "Not enough eyes detected for rotation.\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "('Philip', 9)\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "Not enough eyes detected for rotation.\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "('Philip', 6)\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "('Philip', 8)\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "Not enough eyes detected for rotation.\n",
      "1/1 [==============================] - 0s 251ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "('Philip', 3)\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 151ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "('Philip', 5)\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "('Philip', 3)\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "('Philip', 6)\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 146ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "('Philip', 7)\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 144ms/step\n",
      "('Philip', 4)\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "('Philip', 6)\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "('Philip', 7)\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "('Philip', 2)\n"
     ]
    }
   ],
   "source": [
    "#loop for checking attendance\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    conformation_list=[]\n",
    "    for i in range(10):\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "    \n",
    "        # Get embeddings using the defined function\n",
    "        embeddings = get_embeddings_frame(frame)\n",
    "        if(embeddings is not None):\n",
    "            if database.return_name_max(embeddings)[0]<0.7:\n",
    "                conformation_list.append(database.return_name_max(embeddings)[1])\n",
    "            \n",
    "    \n",
    "        \n",
    "    \n",
    "        cv2.imshow('Frame', frame)\n",
    "    if(len(Counter(conformation_list).most_common(1))!=0):\n",
    "        print(Counter(conformation_list).most_common(1)[0])\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "    \n",
    "            \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf5f0fc-3a3f-4077-bb32-ea5ef05bb08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n"
     ]
    }
   ],
   "source": [
    "#for saving new images to database\n",
    "name=input(\"Enter your name\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        continue\n",
    "\n",
    "    # Get embeddings using the defined function\n",
    "    embeddings = get_embeddings_frame(frame)\n",
    "    if(embeddings is not None):\n",
    "        #print(database.return_name_max(embeddings))\n",
    "        database.create_numpy_frame(embeddings,name)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    cv2.imshow('Frame', frame)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9798e126-ff4e-4bed-ab02-552e7fa53447",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8719ab90-c109-41ca-9def-120ddfbdcfcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82fedab-09d8-4e65-823a-ebf8f7db7490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588cad54-3cf9-47e5-9bb3-2ea8953a3f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484c4e55-ade4-4c07-8adc-9031a1248cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593a877-1f06-4e1f-888c-b1e16256aa54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
